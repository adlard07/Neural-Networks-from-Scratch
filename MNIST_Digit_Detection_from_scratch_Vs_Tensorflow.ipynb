{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QTgpBC5zUajj"
      },
      "outputs": [],
      "source": [
        "# !pip install kaggle\n",
        "# !pip install numpy\n",
        "# !pip install matplotlib\n",
        "# !pip install struct\n",
        "# !pip install array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r-ikCEe9CcTB"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAHN4SGHWd-G"
      },
      "source": [
        "Download MNIST Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLcCb9OjDAhh",
        "outputId": "bb2f15bb-e1d4-4679-b9e5-4bd8d75def1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/hojjatk/mnist-dataset\n",
            "License(s): copyright-authors\n",
            "Downloading mnist-dataset.zip to /content\n",
            " 95% 21.0M/22.0M [00:01<00:00, 32.0MB/s]\n",
            "100% 22.0M/22.0M [00:01<00:00, 19.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d hojjatk/mnist-dataset\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/mnist-dataset.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"mnist-dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF0xeJ_rWh8d"
      },
      "source": [
        "Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dCuyT8S2Sg2N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import struct\n",
        "from array import array\n",
        "from os.path  import join\n",
        "\n",
        "#\n",
        "# MNIST Data Loader Class\n",
        "#\n",
        "class MnistDataloader(object):\n",
        "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
        "                 test_images_filepath, test_labels_filepath):\n",
        "        self.training_images_filepath = training_images_filepath\n",
        "        self.training_labels_filepath = training_labels_filepath\n",
        "        self.test_images_filepath = test_images_filepath\n",
        "        self.test_labels_filepath = test_labels_filepath\n",
        "\n",
        "    def read_images_labels(self, images_filepath, labels_filepath):\n",
        "        labels = []\n",
        "        with open(labels_filepath, 'rb') as file:\n",
        "            magic, size = struct.unpack(\">II\", file.read(8))\n",
        "            if magic != 2049:\n",
        "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
        "            labels = array(\"B\", file.read())\n",
        "\n",
        "        with open(images_filepath, 'rb') as file:\n",
        "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "            if magic != 2051:\n",
        "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
        "            image_data = array(\"B\", file.read())\n",
        "        images = []\n",
        "        for i in range(size):\n",
        "            images.append([0] * rows * cols)\n",
        "        for i in range(size):\n",
        "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
        "            img = img.reshape(28, 28)\n",
        "            images[i][:] = img\n",
        "\n",
        "        return np.array(images), np.array(labels).reshape(-1, 1)\n",
        "\n",
        "    def load_data(self):\n",
        "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
        "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
        "        return (x_train, y_train),(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jZc0vc_CTQIm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import struct\n",
        "from array import array\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "if os.path.exists('mnist-dataset') == False:\n",
        "    with zipfile.ZipFile(\"archive.zip\",\"r\") as zip_ref:\n",
        "        zip_ref.extractall(\"mnist-dataset\")\n",
        "\n",
        "#\n",
        "# MNIST Data Loader Class\n",
        "#\n",
        "class MnistDataloader(object):\n",
        "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
        "                 test_images_filepath, test_labels_filepath):\n",
        "        self.training_images_filepath = training_images_filepath\n",
        "        self.training_labels_filepath = training_labels_filepath\n",
        "        self.test_images_filepath = test_images_filepath\n",
        "        self.test_labels_filepath = test_labels_filepath\n",
        "\n",
        "    def read_images_labels(self, images_filepath, labels_filepath):\n",
        "        labels = []\n",
        "        with open(labels_filepath, 'rb') as file:\n",
        "            magic, size = struct.unpack(\">II\", file.read(8))\n",
        "            if magic != 2049:\n",
        "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
        "            labels = array(\"B\", file.read())\n",
        "\n",
        "        with open(images_filepath, 'rb') as file:\n",
        "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "            if magic != 2051:\n",
        "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
        "            image_data = array(\"B\", file.read())\n",
        "        images = []\n",
        "        for i in range(size):\n",
        "            images.append([0] * rows * cols)\n",
        "        for i in range(size):\n",
        "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
        "            img = img.reshape(28, 28)\n",
        "            images[i][:] = img\n",
        "\n",
        "        return np.array(images), np.array(labels).reshape(-1, 1)\n",
        "\n",
        "    def load_data(self):\n",
        "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
        "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
        "        return (x_train, y_train),(x_test, y_test)\n",
        "\n",
        "def read_data():\n",
        "    print('Data ingestion started ->')\n",
        "    #\n",
        "    # Verify Reading Dataset via MnistDataloader class\n",
        "    # Set file paths based on added MNIST Datasets\n",
        "    #\n",
        "    input_path = 'mnist-dataset'\n",
        "    training_images_filepath = os.path.join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
        "    training_labels_filepath = os.path.join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
        "    test_images_filepath = os.path.join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
        "    test_labels_filepath = os.path.join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
        "\n",
        "    #\n",
        "    # Load MINST dataset\n",
        "    #\n",
        "    mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
        "    (x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
        "\n",
        "    print('Data ingestion complete!->')\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ3lmzT8sdZn"
      },
      "source": [
        "Model Development from scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ujk0dBDGXQpF"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class NeuralNet:\n",
        "    x: np.ndarray\n",
        "    y: np.ndarray\n",
        "\n",
        "    input_size: int\n",
        "    hidden_layers: list\n",
        "    output_size: int\n",
        "\n",
        "    activations: list[str] = field(default_factory=list)\n",
        "    weights: list = field(default_factory=list)\n",
        "    biases: list = field(default_factory=list)\n",
        "    learning_rate: float = 0.01\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Flatten the input if it's multi-dimensional\n",
        "        if len(self.x.shape) > 2:\n",
        "            self.x = self.x.reshape(self.x.shape[0], -1)\n",
        "            # Update input size to match flattened dimension\n",
        "            self.input_size = self.x.shape[1]\n",
        "            print(f\"Input data flattened to shape {self.x.shape}\")\n",
        "\n",
        "        # convert y to one-hot encoding if needed\n",
        "        if len(self.y.shape) == 1 or self.y.shape[1] == 1:\n",
        "            num_classes = max(self.y.flatten()) + 1\n",
        "            y_one_hot = np.zeros((len(self.y), num_classes))\n",
        "            y_one_hot[np.arange(len(self.y)), self.y.flatten().astype(int)] = 1\n",
        "            self.y = y_one_hot\n",
        "\n",
        "        # update output size to match y shape if needed\n",
        "        if self.output_size != self.y.shape[1]:\n",
        "            self.output_size = self.y.shape[1]\n",
        "            print(f\"Output size adjusted to {self.output_size} to match target shape\")\n",
        "\n",
        "        # initialising list of all layers\n",
        "        self.layer_sizes = [self.input_size] + self.hidden_layers + [self.output_size]\n",
        "\n",
        "        # initializing weights and biases with correct shapes\n",
        "        for i in range(len(self.layer_sizes) - 1):\n",
        "            self.weights.append(np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * np.sqrt(2.0 / self.layer_sizes[i]))\n",
        "            self.biases.append(np.zeros((1, self.layer_sizes[i + 1])))\n",
        "\n",
        "    def activation_func(self, inputs, activation_func):\n",
        "        if activation_func == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-inputs))\n",
        "        if activation_func == 'softmax':\n",
        "            return self.softmax(inputs)\n",
        "        if activation_func == 'tanh':\n",
        "            return np.tanh(inputs)\n",
        "        if activation_func == 'relu':\n",
        "            return np.maximum(0, inputs)\n",
        "        if activation_func == 'leaky_relu':\n",
        "            return np.maximum(0.01 * inputs, inputs)\n",
        "\n",
        "    def activation_derivative(self, inputs, func):\n",
        "        if func == 'sigmoid':\n",
        "            return inputs * (1 - inputs)\n",
        "        if func == 'softmax':\n",
        "            return 1\n",
        "        if func == 'tanh':\n",
        "            return 1 - inputs ** 2\n",
        "        if func == 'relu':\n",
        "            return np.where(inputs > 0, 1, 0)\n",
        "        if func == 'leaky_relu':\n",
        "            return np.where(inputs > 0, 1, 0.01)\n",
        "\n",
        "    def softmax(self, inputs):\n",
        "        inputs = np.clip(inputs, -500, 500)\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "    def neuron_activation(self, inputs, weights, bias, activation_func):\n",
        "        return self.activation_func(np.dot(inputs, weights) + bias, activation_func=activation_func)\n",
        "\n",
        "    def cross_entropy_loss(self, y_true, y_pred):\n",
        "        epsilon = 1e-15\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "\n",
        "    def feedforward(self):\n",
        "        self.layer_outputs = [self.x]\n",
        "        current_input = self.x\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            current_input = self.neuron_activation(\n",
        "                current_input,\n",
        "                weights=self.weights[i],\n",
        "                bias=self.biases[i],\n",
        "                activation_func=self.activations[i]\n",
        "            )\n",
        "            self.layer_outputs.append(current_input)\n",
        "\n",
        "        return current_input\n",
        "\n",
        "    def backpropagation(self):\n",
        "        # Forward pass\n",
        "        self.y_pred = self.feedforward()\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = self.cross_entropy_loss(self.y, self.y_pred)\n",
        "        batch_size = self.y.shape[0]\n",
        "        deltas = [(self.y_pred - self.y) / batch_size]\n",
        "\n",
        "        # Backpropagate through layers\n",
        "        for i in reversed(range(len(self.weights) - 1)):\n",
        "            delta = np.dot(deltas[0], self.weights[i + 1].T) * self.activation_derivative(\n",
        "                self.layer_outputs[i + 1], self.activations[i])\n",
        "            deltas.insert(0, delta)\n",
        "\n",
        "        # Update weights and biases\n",
        "        for i in range(len(self.weights)):\n",
        "            d_weight = np.dot(self.layer_outputs[i].T, deltas[i])\n",
        "            d_bias = np.sum(deltas[i], axis=0, keepdims=True)\n",
        "\n",
        "            # Clip gradients to prevent exploding gradients\n",
        "            d_weight = np.clip(d_weight, -1.0, 1.0)\n",
        "            d_bias = np.clip(d_bias, -1.0, 1.0)\n",
        "\n",
        "            self.weights[i] -= self.learning_rate * d_weight\n",
        "            self.biases[i] -= self.learning_rate * d_bias\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def fit(self, epochs):\n",
        "        losses = []\n",
        "        for i in range(1, epochs + 1):\n",
        "            loss = self.backpropagation()\n",
        "            losses.append(loss)\n",
        "            print(f\"Epoch {i}/{epochs} - Loss: {loss:.8f}\")\n",
        "        return losses\n",
        "\n",
        "    def predict(self, X):\n",
        "        if len(X.shape) > 2:\n",
        "            X = X.reshape(X.shape[0], -1)\n",
        "\n",
        "        original_x = self.x\n",
        "        self.x = X\n",
        "        predictions = self.feedforward()\n",
        "        self.x = original_x\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAKpbubGs0yu",
        "outputId": "8fb7d8dd-60a5-4f96-8369-df8d7e90a895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data ingestion started ->\n",
            "Data ingestion complete!->\n",
            "Input shape -> (60000, 28, 28)\n",
            "Data shuffled ->\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "(x_train, y_train), (x_test, y_test) = read_data()\n",
        "print(f\"Input shape -> {x_train.shape}\")\n",
        "\n",
        "# Shuffle data while keeping images and labels together\n",
        "train_indices = np.arange(len(x_train))\n",
        "test_indices = np.arange(len(x_test))\n",
        "\n",
        "np.random.shuffle(train_indices)\n",
        "np.random.shuffle(test_indices)\n",
        "\n",
        "x_train, y_train = x_train[train_indices], y_train[train_indices]\n",
        "x_test, y_test = x_test[test_indices], y_test[test_indices]\n",
        "\n",
        "print('Data shuffled ->')\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.astype(np.float32) / 255.0\n",
        "x_test = x_test.astype(np.float32) / 255.0\n",
        "\n",
        "# Ensure labels are one-hot encoded for the network\n",
        "num_classes = 10\n",
        "y_train_one_hot = np.zeros((len(y_train), num_classes))\n",
        "y_train_one_hot[np.arange(len(y_train)), y_train] = 1\n",
        "\n",
        "y_test_one_hot = np.zeros((len(y_test), num_classes))\n",
        "y_test_one_hot[np.arange(len(y_test)), y_test] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hxJOCKakWb_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf1c756-1ccd-4b99-907b-ea534081d7f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: (60000, 28, 28)\n",
            "Output: (60000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\", x_train.shape)\n",
        "print(\"Output:\", y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HTHzgOhO1ZrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22416599-8c24-4969-cac9-55dfda2e2229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data flattened to shape (60000, 784)\n",
            "Epoch 1/100 - Loss: 23.26153206\n",
            "Epoch 2/100 - Loss: 23.39531677\n",
            "Epoch 3/100 - Loss: 23.69723369\n",
            "Epoch 4/100 - Loss: 24.28743093\n",
            "Epoch 5/100 - Loss: 25.53152323\n",
            "Epoch 6/100 - Loss: 28.20032981\n",
            "Epoch 7/100 - Loss: 33.27750712\n",
            "Epoch 8/100 - Loss: 41.87138661\n",
            "Epoch 9/100 - Loss: 55.92243615\n",
            "Epoch 10/100 - Loss: 78.57556418\n",
            "Epoch 11/100 - Loss: 111.85135250\n",
            "Epoch 12/100 - Loss: 153.84601015\n",
            "Epoch 13/100 - Loss: 195.10487634\n",
            "Epoch 14/100 - Loss: 225.00487555\n",
            "Epoch 15/100 - Loss: 247.11804948\n",
            "Epoch 16/100 - Loss: 262.44829516\n",
            "Epoch 17/100 - Loss: 271.43088402\n",
            "Epoch 18/100 - Loss: 274.82696031\n",
            "Epoch 19/100 - Loss: 264.20136445\n",
            "Epoch 20/100 - Loss: 221.19466354\n",
            "Epoch 21/100 - Loss: 151.22900279\n",
            "Epoch 22/100 - Loss: 87.70849890\n",
            "Epoch 23/100 - Loss: 50.04686503\n",
            "Epoch 24/100 - Loss: 32.71370129\n",
            "Epoch 25/100 - Loss: 25.78906021\n",
            "Epoch 26/100 - Loss: 23.62980572\n",
            "Epoch 27/100 - Loss: 23.14611186\n",
            "Epoch 28/100 - Loss: 23.03917824\n",
            "Epoch 29/100 - Loss: 23.02587729\n",
            "Epoch 30/100 - Loss: 23.02585093\n",
            "Epoch 31/100 - Loss: 23.02585093\n",
            "Epoch 32/100 - Loss: 23.02585093\n",
            "Epoch 33/100 - Loss: 23.02585093\n",
            "Epoch 34/100 - Loss: 23.02585093\n",
            "Epoch 35/100 - Loss: 23.02585093\n",
            "Epoch 36/100 - Loss: 23.02585093\n",
            "Epoch 37/100 - Loss: 23.02585093\n",
            "Epoch 38/100 - Loss: 23.02585093\n",
            "Epoch 39/100 - Loss: 23.02585093\n",
            "Epoch 40/100 - Loss: 23.02585093\n",
            "Epoch 41/100 - Loss: 23.02585093\n",
            "Epoch 42/100 - Loss: 23.02585093\n",
            "Epoch 43/100 - Loss: 23.02585093\n",
            "Epoch 44/100 - Loss: 23.02585093\n",
            "Epoch 45/100 - Loss: 23.02585093\n",
            "Epoch 46/100 - Loss: 23.02585093\n",
            "Epoch 47/100 - Loss: 23.02585093\n",
            "Epoch 48/100 - Loss: 23.02585093\n",
            "Epoch 49/100 - Loss: 23.02585093\n",
            "Epoch 50/100 - Loss: 23.02585093\n",
            "Epoch 51/100 - Loss: 23.02585093\n",
            "Epoch 52/100 - Loss: 23.02585093\n",
            "Epoch 53/100 - Loss: 23.02585093\n",
            "Epoch 54/100 - Loss: 23.02585093\n",
            "Epoch 55/100 - Loss: 23.02585093\n",
            "Epoch 56/100 - Loss: 23.02585093\n",
            "Epoch 57/100 - Loss: 23.02585093\n",
            "Epoch 58/100 - Loss: 23.02585093\n",
            "Epoch 59/100 - Loss: 23.02585093\n",
            "Epoch 60/100 - Loss: 23.02585093\n",
            "Epoch 61/100 - Loss: 23.02585093\n",
            "Epoch 62/100 - Loss: 23.02585093\n",
            "Epoch 63/100 - Loss: 23.02585093\n",
            "Epoch 64/100 - Loss: 23.02585093\n",
            "Epoch 65/100 - Loss: 23.02585093\n",
            "Epoch 66/100 - Loss: 23.02585093\n",
            "Epoch 67/100 - Loss: 23.02585093\n",
            "Epoch 68/100 - Loss: 23.02585093\n",
            "Epoch 69/100 - Loss: 23.02585093\n",
            "Epoch 70/100 - Loss: 23.02585093\n",
            "Epoch 71/100 - Loss: 23.02585093\n",
            "Epoch 72/100 - Loss: 23.02585093\n",
            "Epoch 73/100 - Loss: 23.02585093\n",
            "Epoch 74/100 - Loss: 23.02585093\n",
            "Epoch 75/100 - Loss: 23.02585093\n",
            "Epoch 76/100 - Loss: 23.02585093\n",
            "Epoch 77/100 - Loss: 23.02585093\n",
            "Epoch 78/100 - Loss: 23.02585093\n",
            "Epoch 79/100 - Loss: 23.02585093\n",
            "Epoch 80/100 - Loss: 23.02585093\n",
            "Epoch 81/100 - Loss: 23.02585093\n",
            "Epoch 82/100 - Loss: 23.02585093\n",
            "Epoch 83/100 - Loss: 23.02585093\n",
            "Epoch 84/100 - Loss: 23.02585093\n",
            "Epoch 85/100 - Loss: 23.02585093\n",
            "Epoch 86/100 - Loss: 23.02585093\n",
            "Epoch 87/100 - Loss: 23.02585093\n",
            "Epoch 88/100 - Loss: 23.02585093\n",
            "Epoch 89/100 - Loss: 23.02585093\n",
            "Epoch 90/100 - Loss: 23.02585093\n",
            "Epoch 91/100 - Loss: 23.02585093\n",
            "Epoch 92/100 - Loss: 23.02585093\n",
            "Epoch 93/100 - Loss: 23.02585093\n",
            "Epoch 94/100 - Loss: 23.02585093\n",
            "Epoch 95/100 - Loss: 23.02585093\n",
            "Epoch 96/100 - Loss: 23.02585093\n",
            "Epoch 97/100 - Loss: 23.02585093\n",
            "Epoch 98/100 - Loss: 23.02585093\n",
            "Epoch 99/100 - Loss: 23.02585093\n",
            "Epoch 100/100 - Loss: 23.02585093\n",
            "Epoch 1 - Loss: 23.26153206\n",
            "Epoch 2 - Loss: 23.39531677\n",
            "Epoch 3 - Loss: 23.69723369\n",
            "Epoch 4 - Loss: 24.28743093\n",
            "Epoch 5 - Loss: 25.53152323\n",
            "Epoch 6 - Loss: 28.20032981\n",
            "Epoch 7 - Loss: 33.27750712\n",
            "Epoch 8 - Loss: 41.87138661\n",
            "Epoch 9 - Loss: 55.92243615\n",
            "Epoch 10 - Loss: 78.57556418\n",
            "Epoch 11 - Loss: 111.85135250\n",
            "Epoch 12 - Loss: 153.84601015\n",
            "Epoch 13 - Loss: 195.10487634\n",
            "Epoch 14 - Loss: 225.00487555\n",
            "Epoch 15 - Loss: 247.11804948\n",
            "Epoch 16 - Loss: 262.44829516\n",
            "Epoch 17 - Loss: 271.43088402\n",
            "Epoch 18 - Loss: 274.82696031\n",
            "Epoch 19 - Loss: 264.20136445\n",
            "Epoch 20 - Loss: 221.19466354\n",
            "Epoch 21 - Loss: 151.22900279\n",
            "Epoch 22 - Loss: 87.70849890\n",
            "Epoch 23 - Loss: 50.04686503\n",
            "Epoch 24 - Loss: 32.71370129\n",
            "Epoch 25 - Loss: 25.78906021\n",
            "Epoch 26 - Loss: 23.62980572\n",
            "Epoch 27 - Loss: 23.14611186\n",
            "Epoch 28 - Loss: 23.03917824\n",
            "Epoch 29 - Loss: 23.02587729\n",
            "Epoch 30 - Loss: 23.02585093\n",
            "Epoch 31 - Loss: 23.02585093\n",
            "Epoch 32 - Loss: 23.02585093\n",
            "Epoch 33 - Loss: 23.02585093\n",
            "Epoch 34 - Loss: 23.02585093\n",
            "Epoch 35 - Loss: 23.02585093\n",
            "Epoch 36 - Loss: 23.02585093\n",
            "Epoch 37 - Loss: 23.02585093\n",
            "Epoch 38 - Loss: 23.02585093\n",
            "Epoch 39 - Loss: 23.02585093\n",
            "Epoch 40 - Loss: 23.02585093\n",
            "Epoch 41 - Loss: 23.02585093\n",
            "Epoch 42 - Loss: 23.02585093\n",
            "Epoch 43 - Loss: 23.02585093\n",
            "Epoch 44 - Loss: 23.02585093\n",
            "Epoch 45 - Loss: 23.02585093\n",
            "Epoch 46 - Loss: 23.02585093\n",
            "Epoch 47 - Loss: 23.02585093\n",
            "Epoch 48 - Loss: 23.02585093\n",
            "Epoch 49 - Loss: 23.02585093\n",
            "Epoch 50 - Loss: 23.02585093\n",
            "Epoch 51 - Loss: 23.02585093\n",
            "Epoch 52 - Loss: 23.02585093\n",
            "Epoch 53 - Loss: 23.02585093\n",
            "Epoch 54 - Loss: 23.02585093\n",
            "Epoch 55 - Loss: 23.02585093\n",
            "Epoch 56 - Loss: 23.02585093\n",
            "Epoch 57 - Loss: 23.02585093\n",
            "Epoch 58 - Loss: 23.02585093\n",
            "Epoch 59 - Loss: 23.02585093\n",
            "Epoch 60 - Loss: 23.02585093\n",
            "Epoch 61 - Loss: 23.02585093\n",
            "Epoch 62 - Loss: 23.02585093\n",
            "Epoch 63 - Loss: 23.02585093\n",
            "Epoch 64 - Loss: 23.02585093\n",
            "Epoch 65 - Loss: 23.02585093\n",
            "Epoch 66 - Loss: 23.02585093\n",
            "Epoch 67 - Loss: 23.02585093\n",
            "Epoch 68 - Loss: 23.02585093\n",
            "Epoch 69 - Loss: 23.02585093\n",
            "Epoch 70 - Loss: 23.02585093\n",
            "Epoch 71 - Loss: 23.02585093\n",
            "Epoch 72 - Loss: 23.02585093\n",
            "Epoch 73 - Loss: 23.02585093\n",
            "Epoch 74 - Loss: 23.02585093\n",
            "Epoch 75 - Loss: 23.02585093\n",
            "Epoch 76 - Loss: 23.02585093\n",
            "Epoch 77 - Loss: 23.02585093\n",
            "Epoch 78 - Loss: 23.02585093\n",
            "Epoch 79 - Loss: 23.02585093\n",
            "Epoch 80 - Loss: 23.02585093\n",
            "Epoch 81 - Loss: 23.02585093\n",
            "Epoch 82 - Loss: 23.02585093\n",
            "Epoch 83 - Loss: 23.02585093\n",
            "Epoch 84 - Loss: 23.02585093\n",
            "Epoch 85 - Loss: 23.02585093\n",
            "Epoch 86 - Loss: 23.02585093\n",
            "Epoch 87 - Loss: 23.02585093\n",
            "Epoch 88 - Loss: 23.02585093\n",
            "Epoch 89 - Loss: 23.02585093\n",
            "Epoch 90 - Loss: 23.02585093\n",
            "Epoch 91 - Loss: 23.02585093\n",
            "Epoch 92 - Loss: 23.02585093\n",
            "Epoch 93 - Loss: 23.02585093\n",
            "Epoch 94 - Loss: 23.02585093\n",
            "Epoch 95 - Loss: 23.02585093\n",
            "Epoch 96 - Loss: 23.02585093\n",
            "Epoch 97 - Loss: 23.02585093\n",
            "Epoch 98 - Loss: 23.02585093\n",
            "Epoch 99 - Loss: 23.02585093\n",
            "Epoch 100 - Loss: 23.02585093\n"
          ]
        }
      ],
      "source": [
        "# Define network parameters\n",
        "hidden_layers = [32, 64, 32]\n",
        "activations = ['relu', 'relu', 'relu', 'softmax']\n",
        "\n",
        "# Reduced learning rate for better convergence\n",
        "nn = NeuralNet(\n",
        "    x=x_train,\n",
        "    y=y_train_one_hot,\n",
        "    input_size=784,\n",
        "    hidden_layers=hidden_layers,\n",
        "    activations=activations,\n",
        "    output_size=10,\n",
        "    learning_rate=0.01  # Changed to a lower learning rate\n",
        ")\n",
        "\n",
        "# Train the network\n",
        "losses = nn.fit(epochs=100)\n",
        "\n",
        "# Logging the loss values\n",
        "for epoch, loss in enumerate(losses, 1):\n",
        "    print(f\"Epoch {epoch} - Loss: {loss:.8f}\")\n",
        "\n",
        "# Training set predictions\n",
        "train_predictions = nn.predict(x_train)\n",
        "train_predicted_classes = np.argmax(train_predictions, axis=1)\n",
        "\n",
        "# Test set predictions\n",
        "test_predictions = nn.predict(x_test)\n",
        "test_predicted_classes = np.argmax(test_predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training results\n",
        "print(\"\\nTraining Set Results:\")\n",
        "correct_train_predictions = 0\n",
        "for i in range(len(train_predicted_classes)):\n",
        "  if  train_predicted_classes[i] == y_train[i]:\n",
        "    correct_train_predictions += 1\n",
        "train_accuracy = (correct_train_predictions / len(y_train) * 100)\n",
        "print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "# test results\n",
        "print(\"\\nTest Set Results:\")\n",
        "correct_test_predictions = 0\n",
        "for i in range(len(test_predicted_classes)):\n",
        "  if  test_predicted_classes[i] == y_test[i]:\n",
        "    correct_test_predictions += 1\n",
        "test_accuracy = (correct_test_predictions / len(y_test) * 100)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "print(correct_train_predictions)\n",
        "print(correct_test_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELpdLmtf1F98",
        "outputId": "230ff1cd-0f2f-4cdb-97ae-b2b8fade8370"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Set Results:\n",
            "Training Accuracy: 9.87%\n",
            "\n",
            "Test Set Results:\n",
            "Test Accuracy: 9.80%\n",
            "5923\n",
            "980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "7y1YnlpUIpTx",
        "outputId": "a89d3483-57d0-4ff1-f603-5ba55581b7df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow"
      ],
      "metadata": {
        "id": "bZrboW3b2lQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(32, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "NsCQUgaPz4vQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=100)"
      ],
      "metadata": {
        "id": "0vV3sMcpykle",
        "outputId": "f628c08f-a091-4995-d480-c15ad684a709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8041 - loss: 0.6973\n",
            "Epoch 2/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9489 - loss: 0.1753\n",
            "Epoch 3/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9613 - loss: 0.1309\n",
            "Epoch 4/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9674 - loss: 0.1094\n",
            "Epoch 5/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9718 - loss: 0.0931\n",
            "Epoch 6/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9759 - loss: 0.0787\n",
            "Epoch 7/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.0709\n",
            "Epoch 8/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9800 - loss: 0.0629\n",
            "Epoch 9/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9813 - loss: 0.0573\n",
            "Epoch 10/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9837 - loss: 0.0513\n",
            "Epoch 11/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9860 - loss: 0.0456\n",
            "Epoch 12/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9869 - loss: 0.0418\n",
            "Epoch 13/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9878 - loss: 0.0371\n",
            "Epoch 14/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9875 - loss: 0.0374\n",
            "Epoch 15/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9889 - loss: 0.0325\n",
            "Epoch 16/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9898 - loss: 0.0295\n",
            "Epoch 17/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0273\n",
            "Epoch 18/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9904 - loss: 0.0281\n",
            "Epoch 19/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9913 - loss: 0.0255\n",
            "Epoch 20/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0207\n",
            "Epoch 21/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9928 - loss: 0.0219\n",
            "Epoch 22/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0196\n",
            "Epoch 23/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9940 - loss: 0.0184\n",
            "Epoch 24/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0150\n",
            "Epoch 25/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9936 - loss: 0.0189\n",
            "Epoch 26/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9947 - loss: 0.0160\n",
            "Epoch 27/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0150\n",
            "Epoch 28/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9952 - loss: 0.0149\n",
            "Epoch 29/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0141\n",
            "Epoch 30/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9953 - loss: 0.0134\n",
            "Epoch 31/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0136\n",
            "Epoch 32/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9949 - loss: 0.0157\n",
            "Epoch 33/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9960 - loss: 0.0124\n",
            "Epoch 34/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9957 - loss: 0.0129\n",
            "Epoch 35/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0116\n",
            "Epoch 36/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9961 - loss: 0.0118\n",
            "Epoch 37/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9966 - loss: 0.0106\n",
            "Epoch 38/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9962 - loss: 0.0113\n",
            "Epoch 39/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9958 - loss: 0.0119\n",
            "Epoch 40/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9960 - loss: 0.0123\n",
            "Epoch 41/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9975 - loss: 0.0081\n",
            "Epoch 42/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9964 - loss: 0.0111\n",
            "Epoch 43/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9969 - loss: 0.0088\n",
            "Epoch 44/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9957 - loss: 0.0128\n",
            "Epoch 45/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9966 - loss: 0.0092\n",
            "Epoch 46/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0061\n",
            "Epoch 47/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0114\n",
            "Epoch 48/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9974 - loss: 0.0074\n",
            "Epoch 49/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9981 - loss: 0.0064\n",
            "Epoch 50/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 0.0094\n",
            "Epoch 51/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0086\n",
            "Epoch 52/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9968 - loss: 0.0094\n",
            "Epoch 53/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0046\n",
            "Epoch 54/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9970 - loss: 0.0097\n",
            "Epoch 55/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0110\n",
            "Epoch 56/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 0.0095\n",
            "Epoch 57/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9969 - loss: 0.0097\n",
            "Epoch 58/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0068\n",
            "Epoch 59/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0063\n",
            "Epoch 60/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9978 - loss: 0.0069\n",
            "Epoch 61/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0056\n",
            "Epoch 62/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9975 - loss: 0.0071\n",
            "Epoch 63/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9969 - loss: 0.0092\n",
            "Epoch 64/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9978 - loss: 0.0075\n",
            "Epoch 65/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 0.0089\n",
            "Epoch 66/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0069\n",
            "Epoch 67/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0072\n",
            "Epoch 68/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0069\n",
            "Epoch 69/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0058\n",
            "Epoch 70/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0068\n",
            "Epoch 71/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9978 - loss: 0.0077\n",
            "Epoch 72/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9980 - loss: 0.0060\n",
            "Epoch 73/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9981 - loss: 0.0067\n",
            "Epoch 74/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9971 - loss: 0.0101\n",
            "Epoch 75/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9982 - loss: 0.0057\n",
            "Epoch 76/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9977 - loss: 0.0071\n",
            "Epoch 77/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0047\n",
            "Epoch 78/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0080\n",
            "Epoch 79/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0052\n",
            "Epoch 80/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0062\n",
            "Epoch 81/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0062\n",
            "Epoch 82/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0070\n",
            "Epoch 83/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0053\n",
            "Epoch 84/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0057\n",
            "Epoch 85/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0088\n",
            "Epoch 86/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0075\n",
            "Epoch 87/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9972 - loss: 0.0098\n",
            "Epoch 88/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0043\n",
            "Epoch 89/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0058\n",
            "Epoch 90/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9979 - loss: 0.0061\n",
            "Epoch 91/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0069\n",
            "Epoch 92/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0094\n",
            "Epoch 93/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0065\n",
            "Epoch 94/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0070\n",
            "Epoch 95/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0068\n",
            "Epoch 96/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9980 - loss: 0.0081\n",
            "Epoch 97/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0041\n",
            "Epoch 98/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0061\n",
            "Epoch 99/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9986 - loss: 0.0047\n",
            "Epoch 100/100\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0091\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ea550207550>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "eEJTUHt7yzde",
        "outputId": "bc8cc88a-c94e-4550-aa89-43bf464374c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9706 - loss: 0.3524\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33195847272872925, 0.9700999855995178]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GnCmzmHE0YG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}